{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "878db32d",
   "metadata": {},
   "source": [
    "## Preprocessing for Stan Modeling\n",
    "**Note:** This is a tutorial notebook that walks through the first half of the `mcmcjob.py` script. Although this notebook can be used to preprare raw data for Stan modeling on a personal machine, it cannot be used on the HBS cluster - juypter notebooks are not supported on the cluster at this time. Therefore, please use the `mcmcjob.py` script when submitting jobs on the cluster. This notebook is intended as a reference.\n",
    "\n",
    "### Step 1: Import Libraries & Data\n",
    "The `mcmcjob.py` script requires the following libraries:\n",
    "* pickle\n",
    "* os\n",
    "* sys\n",
    "* numpy\n",
    "* pandas\n",
    "* random\n",
    "\n",
    "Most python users should be familiar with most of the libraries with the possible exception of pickle. Pickle will be used to serialize our model code, fit, and data into a byte structure for saving. Let's import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "912c2fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\") # this is unnecessary, but included to avoid things like copywarnings in output.\n",
    "# Feel free to comment out this warnings line if you'd like to see warnings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01ac43",
   "metadata": {},
   "source": [
    "Next we'll load the data. The data should be saved in the same working directory with `mcmcjob.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf8eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(\"final_dataset_for_ddm_dec_21.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731dd88",
   "metadata": {},
   "source": [
    "We will build a new dataframe that contains only the variables that we need for the Stan model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "003fbb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'ID': raw['Random.ID'], 'choice': raw['emo_binary'],\n",
    "                   'rt': raw['rt'], 'valence': raw['valence'],\n",
    "                   'identity': raw['faces'], 'intensity': raw['valence_values'],\n",
    "                   'ratio': raw['b_person_ratio']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554d3a0",
   "metadata": {},
   "source": [
    "### Step 2: Preprocessing - Converting Strings to Integers\n",
    "The Stan language can be peculiar. We need to modify our raw data for interpretation with Stan. First we will drop any trials with NAs, NANs, or missing data. Then we will convert the data in our **choice** and **valence** columns from strings to integers. Unlike python, there are no statements with strings in Stan (e.g., `if value == 'string':`). In order to write conditional statements into our model, we have to convert these strings to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e743425",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0).reset_index(drop=True)\n",
    "df['choice'] = [1 if x == 'Not Emotional' else 2 for x in df['choice']]\n",
    "df['valence'] = [1 if x == 'Happy' else 2 for x in df['valence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c52afca",
   "metadata": {},
   "source": [
    "Here, we specified that all 'Not Emotional' cells in the **choice** column will instead read $1$. Otherwise, cells will read $2$, as is the case with 'Emotional' judgments. We similarly convert strings to integers in the **valence** column.\n",
    "\n",
    "Next we need to convert the strings in the **identity** and **intensity** columns to lists. We will do this by spliting the strings along the separator ', ':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb5171b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['identity'] = [x.split(', ') for x in df['identity']]\n",
    "df['intensity'] =  [x.split(', ') for x in df['intensity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f32ae4",
   "metadata": {},
   "source": [
    "Although **intensity** is now a column of lists with integers as elements, which Stan can work with, **identity** still has strings as elements. We need to convert these strings to integers. While we are doing this, we will also force all lists to be length 12. Why are we doing this? Again, this part is not necessary, but the way that I wrote the Stan model doesn't acknowledge differences in array size; if we don't acknowledge this and yet feed in trials with differing sizes, the model will disproportionately estimate weight to the stimuli in the trials with smaller arrays. Our goal with this modeling analysis is to estimate a unit of transformation from intensity to evidence, and we do not want to bias that estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b47e6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "identitydict = {'E': 1, 'F': 1, 'B': 2, 'C': 2, 'NA': 0}\n",
    "for i, x in enumerate(df['identity']):\n",
    "    while len(x) < 12:\n",
    "        x.append('NA')\n",
    "        df['intensity'][i].append(0)\n",
    "    df['identity'][i] = [identitydict[e] for e in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6488ce86",
   "metadata": {},
   "source": [
    "Notice that we collapsed all identities to be either Black (1) or White (2). We could always make a more complicated model that delineates the faces within these groups, but we chose to collapse for simplicity. Also note that in our efforts to make all lists length $12$, we appended $0$s to the intensity lists and NAs (0) to the identity lists.\n",
    "\n",
    "### Step 3: Preprocessing - Additional Modification\n",
    "\n",
    "Although our extended DDM will use **intensity** and **identity** to calculate drift rate, most of our simpler models will only use **valence** and **ratio**. Next, we will create a simple index variable for keeping track of the various **valence** and **ratio** conditions for some of these models. You might ask, why do we need these variables if we're simply going to index over them? We don't actually need them, but since an index is less interpretable to the uninformed, it is nice to include the variables anyway so that they can be referenced in our model object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfd3dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['indexer'] = [4*df['ratio'][i] + 3*(df['valence'][i]-1) for i, x in enumerate(df['ID'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf2fecc",
   "metadata": {},
   "source": [
    "What this **indexer** does is convert our **ratio** and **valence** integers into $6$ discrete indexing integers. See for yourself by plugging in values (e.g., where ratio is $0.50$ and valence is $2$, indexer is $4(0.50) + 3(2-1) = 5$). Each of these six indices represents one of our six conditions (e.g., Happy-25%Black, Angry-75%Black, etc.). Note that **indexer** starts at $1$ because Stan uses 1-based indexing (unlike python's 0-based indexing).\n",
    "\n",
    "Now it is time for some data cleaning. This is primarily removing subjects that lack variability in their choices. I'm being particularly liberal with inclusion, removing only those subjects that make the same decision for *every* trial. You could rewrite this loop to search for subjects with more conservative criteria (i.e., 9:1 emotional decisions to non-emotional). We will print the IDs that need to be excluded due to low variability for our records. If the printed list is empty, congratulations, you have no subjects that need excluding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14aa0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects with no variation: []\n"
     ]
    }
   ],
   "source": [
    "dellist = []\n",
    "for x in df['ID'].unique():\n",
    "    if len(df[df['ID']==x]['choice'].unique()) < 2:\n",
    "        dellist.append(x)\n",
    "print('subjects with no variation: %s' % dellist)\n",
    "df = df[~df['ID'].isin(dellist)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb979741",
   "metadata": {},
   "source": [
    "Finally, we need to make some small modifications to our response time variable **rt**. First, we will divide each value in the **rt** column by $1000$ so that we're working in seconds rather than milliseconds. This is not a necessary step for working in Stan, rather it is necessary because of the way I wrote the Stan models we will use (e.g., I constrain $\\tau$, the non-decision time parameter, to be greater than 0.1, or 100ms. If we fed milliseconds into this model, that lower bound would be misinterpreted as 100us)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f0721ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rt'] = [x/1000 for x in df['rt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002cb41",
   "metadata": {},
   "source": [
    "Second, we will drop all trials where **rt** is less than 0.1 seconds. As previously mentioned, we are constraining non-decision time to be greater than 100ms - anything less than this is likely to be a false start and inappropriate to model with a DDM. We will define this so that we can include it in the data we send to Stan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76605390",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['rt'] > 0.1]\n",
    "df = df.reset_index(drop=True)\n",
    "rtbound = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8ad4c4",
   "metadata": {},
   "source": [
    "### Step 4: Setting up the Data Pickle\n",
    "We are now ready to convert everything - that we just so painstakingly converted to a dataframe - to arrays and vectors. Why didn't we just start with arrays and vectors?! Well, I *like* dataframes... I find them easier to work with. Unfortunately, Stan is peculiar, and will only take integers, real numbers, vectors, and matrices as data. We did all of the preprocessing in a dataframe because that's what I'm most comfortable with, but now it is time to convert that dataframe into something Stan will accept.\n",
    "\n",
    "First, we need a few more bits of information. Namely, the number of subjects, the max number of trials, and the number of trials each subject has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "804e0040",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['ID'], sort=False)\n",
    "trials_per = grouped.size()\n",
    "subs = list(trials_per.index)\n",
    "nsubs = len(subs)\n",
    "tsubs = list(trials_per)\n",
    "tmax = max(tsubs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa14f9b",
   "metadata": {},
   "source": [
    "Next, we will create a bunch of arrays with shapes `nsubs`x`tmax`, or the number of subjects by the max number of trials. These arrays will all be filled with $-1$ values. Why $-1$? It is a placeholder that reflects no data - we will fill in these values with our data later. Note that some of the arrays are 3-dimensional. Those are for data that we need to import as vectors, where there are multiple values present in each trial (i.e., intensities and identities). You'll also see that some arrays are composed of integers and others of floats. These correspond to integers and reals in Stan, and depend on whether floating point numbers are necessary to represent your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f105291",
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = np.full((nsubs, tmax), -1, dtype=int)\n",
    "rt = np.full((nsubs, tmax), -1, dtype=float)\n",
    "valence = np.full((nsubs, tmax), -1, dtype=int)\n",
    "intensity = np.full((nsubs, tmax, 12), -1, dtype=int)\n",
    "identity = np.full((nsubs, tmax, 12), -1, dtype=int)\n",
    "ratio = np.full((nsubs, tmax), -1, dtype=float)\n",
    "indexer = np.full((nsubs, tmax), -1, dtype=int)\n",
    "rtmin = np.full(nsubs, -1, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7daf8ea",
   "metadata": {},
   "source": [
    "Next, we are going to iterate over each subject to fill their data into our newly created arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5e42809",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_group = iter(grouped)\n",
    "for s in range(nsubs):\n",
    "    _, sub_data = next(sub_group)\n",
    "    t = tsubs[s]\n",
    "    choice[s][:t] = sub_data['choice']\n",
    "    rt[s][:t] = sub_data['rt']\n",
    "    valence[s][:t] = sub_data['valence']\n",
    "    intensity[s][:t] = np.asarray([np.array(x) for x in sub_data['intensity']])\n",
    "    identity[s][:t] = np.asarray([np.array(x) for x in sub_data['identity']])\n",
    "    ratio[s][:t] = sub_data['ratio']\n",
    "    indexer[s][:t] = sub_data['indexer']\n",
    "    rtmin[s] = min(sub_data['rt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec20f44",
   "metadata": {},
   "source": [
    "With these arrays we will create a dictionary and save that dictionary to our working directory as a pkl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db7096f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'N': nsubs,\n",
    "    'T': tmax,\n",
    "    'Tsub': tsubs,\n",
    "    'choice': choice,\n",
    "    'valence': valence,\n",
    "    'rt': rt,\n",
    "    'rtmin': rtmin,\n",
    "    'rtbound': rtbound,\n",
    "    'intensity': intensity,\n",
    "    'identity': identity,\n",
    "    'ratio': ratio,\n",
    "    'indexer': indexer,\n",
    "}\n",
    "with open(\"modeldata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f, protocol=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1665b424",
   "metadata": {},
   "source": [
    "And that's it. We now have our data preprocessed for Stan. In the next tutorial, we will be using pystan, an API wrapper around the Stan language which we will use to write and run our models using the data we've prepared. Before you go, take a moment to look through your final data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7749cc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'N': 271,\n",
       " 'T': 50,\n",
       " 'Tsub': [50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  11,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  48,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50,\n",
       "  50],\n",
       " 'choice': array([[2, 2, 2, ..., 2, 2, 2],\n",
       "        [2, 1, 2, ..., 1, 2, 2],\n",
       "        [1, 2, 1, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [2, 2, 2, ..., 1, 1, 2],\n",
       "        [2, 2, 2, ..., 1, 2, 2],\n",
       "        [2, 2, 1, ..., 1, 1, 2]]),\n",
       " 'valence': array([[1, 2, 1, ..., 2, 1, 2],\n",
       "        [1, 2, 2, ..., 2, 2, 2],\n",
       "        [2, 1, 1, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [2, 1, 2, ..., 1, 1, 1],\n",
       "        [1, 2, 1, ..., 1, 2, 2],\n",
       "        [1, 2, 1, ..., 1, 1, 1]]),\n",
       " 'rt': array([[0.934, 0.987, 0.779, ..., 0.703, 0.697, 0.953],\n",
       "        [1.106, 1.177, 1.255, ..., 1.493, 2.047, 3.902],\n",
       "        [0.716, 0.865, 0.725, ..., 0.758, 0.974, 0.884],\n",
       "        ...,\n",
       "        [0.878, 0.524, 0.565, ..., 0.882, 0.711, 1.263],\n",
       "        [0.632, 0.613, 0.644, ..., 1.167, 1.331, 0.882],\n",
       "        [0.757, 0.694, 0.595, ..., 0.874, 0.899, 0.987]]),\n",
       " 'rtmin': array([0.507, 0.759, 0.617, 0.745, 0.621, 0.401, 0.135, 0.695, 0.394,\n",
       "        0.432, 0.535, 0.823, 0.452, 0.403, 0.45 , 0.555, 0.468, 0.561,\n",
       "        0.765, 0.687, 0.36 , 0.59 , 0.432, 0.758, 0.519, 0.52 , 0.467,\n",
       "        0.421, 0.586, 0.394, 0.311, 0.456, 0.639, 0.331, 0.496, 0.872,\n",
       "        0.74 , 0.564, 0.576, 0.602, 0.495, 0.363, 0.802, 0.475, 0.654,\n",
       "        0.521, 0.509, 0.343, 0.624, 0.388, 0.436, 0.531, 0.392, 0.353,\n",
       "        0.524, 0.939, 0.449, 0.479, 0.449, 0.535, 0.512, 0.529, 0.52 ,\n",
       "        0.461, 0.852, 0.553, 0.252, 0.515, 0.41 , 0.414, 0.682, 0.603,\n",
       "        0.517, 0.502, 0.936, 0.467, 0.702, 0.665, 0.612, 0.406, 0.157,\n",
       "        0.141, 0.425, 0.145, 0.63 , 0.655, 0.525, 0.301, 0.616, 0.524,\n",
       "        0.649, 0.523, 0.12 , 0.407, 0.384, 0.434, 0.257, 0.593, 0.582,\n",
       "        0.194, 0.723, 0.945, 0.4  , 0.746, 0.389, 0.334, 0.593, 0.808,\n",
       "        0.602, 0.486, 0.558, 0.45 , 0.577, 0.343, 0.303, 0.595, 0.437,\n",
       "        0.714, 0.531, 0.46 , 0.532, 0.47 , 0.497, 0.927, 0.403, 0.4  ,\n",
       "        0.544, 0.425, 0.535, 0.519, 0.675, 0.53 , 0.353, 0.378, 0.623,\n",
       "        0.55 , 0.573, 0.403, 0.277, 0.441, 0.387, 0.607, 0.38 , 0.498,\n",
       "        0.416, 0.493, 0.434, 0.438, 0.811, 0.43 , 0.197, 0.382, 0.585,\n",
       "        0.54 , 0.434, 0.179, 0.429, 0.635, 0.695, 0.471, 0.895, 0.451,\n",
       "        0.313, 0.372, 0.5  , 0.459, 0.488, 0.289, 0.416, 0.439, 1.   ,\n",
       "        0.397, 0.46 , 0.276, 0.493, 0.462, 0.384, 0.707, 0.639, 0.416,\n",
       "        0.3  , 0.131, 0.472, 0.375, 0.365, 0.474, 0.456, 0.426, 0.518,\n",
       "        0.454, 0.672, 0.577, 0.588, 0.408, 0.612, 0.386, 1.148, 0.755,\n",
       "        0.254, 0.429, 0.404, 0.784, 0.484, 0.42 , 0.43 , 0.119, 0.668,\n",
       "        0.424, 0.735, 0.734, 0.449, 0.31 , 0.545, 0.471, 0.332, 0.631,\n",
       "        0.407, 0.325, 0.734, 0.316, 0.11 , 0.481, 0.481, 0.695, 0.479,\n",
       "        0.834, 0.575, 0.438, 0.56 , 0.643, 0.591, 0.479, 0.285, 0.399,\n",
       "        0.447, 0.624, 0.491, 0.39 , 0.375, 0.49 , 0.508, 0.573, 0.547,\n",
       "        0.389, 0.828, 0.471, 0.662, 0.691, 0.521, 0.409, 0.456, 0.438,\n",
       "        0.377, 0.513, 0.681, 0.482, 0.335, 0.756, 0.498, 0.555, 0.626,\n",
       "        0.615, 0.405, 0.64 , 0.784, 0.439, 0.381, 0.58 , 0.524, 0.57 ,\n",
       "        0.524]),\n",
       " 'rtbound': 0.1,\n",
       " 'intensity': array([[[ 52,  88,  70, ...,   0,   0,   0],\n",
       "         [138, 108, 107, ..., 115, 135, 121],\n",
       "         [ 92,  56,  99, ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [115, 134, 117, ...,   0,   0,   0],\n",
       "         [ 87,  92,  96, ...,   0,   0,   0],\n",
       "         [146, 104, 126, ..., 133, 129, 145]],\n",
       " \n",
       "        [[ 67,  56,  87, ...,   0,   0,   0],\n",
       "         [117, 134, 136, ...,   0,   0,   0],\n",
       "         [116, 134, 127, ..., 133, 122, 148],\n",
       "         ...,\n",
       "         [112, 115, 108, ..., 149, 135, 125],\n",
       "         [131, 101, 102, ...,   0,   0,   0],\n",
       "         [140, 141, 125, ...,   0,   0,   0]],\n",
       " \n",
       "        [[104, 112, 113, ...,   0,   0,   0],\n",
       "         [ 54,  75,  99, ...,   0,   0,   0],\n",
       "         [ 55,  78,  89, ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [ 82,  72,  64, ...,   0,   0,   0],\n",
       "         [ 89,  59,  88, ...,  82,  90,  75],\n",
       "         [ 93,  92,  99, ...,  67,  96,  60]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[103, 113, 106, ...,   0,   0,   0],\n",
       "         [ 75,  69,  92, ...,   0,   0,   0],\n",
       "         [150, 101, 126, ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [ 62,  57,  56, ...,   0,   0,   0],\n",
       "         [ 70,  54,  58, ...,   0,   0,   0],\n",
       "         [ 68,  79,  93, ...,   0,   0,   0]],\n",
       " \n",
       "        [[ 53,  97,  58, ...,   0,   0,   0],\n",
       "         [102, 104, 130, ...,   0,   0,   0],\n",
       "         [ 67,  95,  91, ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [ 97,  72,  74, ...,  63,  76,  69],\n",
       "         [132, 140, 133, ...,   0,   0,   0],\n",
       "         [139, 115, 149, ...,   0,   0,   0]],\n",
       " \n",
       "        [[ 67,  54,  91, ...,   0,   0,   0],\n",
       "         [148, 135, 107, ...,   0,   0,   0],\n",
       "         [ 78,  77,  97, ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [ 62,  69,  96, ...,   0,   0,   0],\n",
       "         [ 89,  90,  54, ...,   0,   0,   0],\n",
       "         [ 57,  84,  53, ...,   0,   0,   0]]]),\n",
       " 'identity': array([[[2, 2, 2, ..., 0, 0, 0],\n",
       "         [2, 1, 2, ..., 2, 2, 2],\n",
       "         [2, 1, 2, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [2, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 2, 1, ..., 0, 0, 0],\n",
       "         [2, 2, 2, ..., 2, 1, 1]],\n",
       " \n",
       "        [[1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 2, 2, ..., 0, 0, 0],\n",
       "         [2, 2, 2, ..., 2, 2, 2],\n",
       "         ...,\n",
       "         [1, 2, 2, ..., 1, 1, 1],\n",
       "         [1, 1, 2, ..., 0, 0, 0],\n",
       "         [1, 1, 2, ..., 0, 0, 0]],\n",
       " \n",
       "        [[1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 2, 1, ..., 0, 0, 0],\n",
       "         [2, 1, 2, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [2, 1, 2, ..., 0, 0, 0],\n",
       "         [2, 2, 1, ..., 1, 1, 1],\n",
       "         [1, 1, 1, ..., 2, 2, 2]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[2, 1, 2, ..., 0, 0, 0],\n",
       "         [2, 1, 2, ..., 0, 0, 0],\n",
       "         [2, 1, 1, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 2, 1, ..., 0, 0, 0],\n",
       "         [2, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0]],\n",
       " \n",
       "        [[1, 2, 1, ..., 0, 0, 0],\n",
       "         [2, 1, 2, ..., 0, 0, 0],\n",
       "         [1, 2, 2, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [2, 2, 2, ..., 1, 2, 2],\n",
       "         [1, 2, 2, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0]],\n",
       " \n",
       "        [[1, 1, 2, ..., 0, 0, 0],\n",
       "         [2, 1, 1, ..., 0, 0, 0],\n",
       "         [2, 2, 1, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [2, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 2, ..., 0, 0, 0],\n",
       "         [1, 2, 1, ..., 0, 0, 0]]]),\n",
       " 'ratio': array([[0.25, 0.25, 0.25, ..., 0.25, 0.75, 0.25],\n",
       "        [0.75, 0.5 , 0.25, ..., 0.5 , 0.75, 0.5 ],\n",
       "        [0.75, 0.25, 0.5 , ..., 0.25, 0.75, 0.25],\n",
       "        ...,\n",
       "        [0.5 , 0.5 , 0.75, ..., 0.75, 0.25, 0.75],\n",
       "        [0.75, 0.25, 0.75, ..., 0.25, 0.5 , 0.75],\n",
       "        [0.75, 0.75, 0.5 , ..., 0.5 , 0.75, 0.75]]),\n",
       " 'indexer': array([[1, 4, 1, ..., 4, 3, 4],\n",
       "        [3, 5, 4, ..., 5, 6, 5],\n",
       "        [6, 1, 2, ..., 1, 3, 1],\n",
       "        ...,\n",
       "        [5, 2, 6, ..., 3, 1, 3],\n",
       "        [3, 4, 3, ..., 1, 5, 6],\n",
       "        [3, 6, 2, ..., 2, 3, 3]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
