{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Stan fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some libraries might not be used for pre-processing and Stan modeling - might be orphans of past notebooks :/\n",
    "from __future__ import print_function, division\n",
    "import getpass\n",
    "import pickle\n",
    "import pathlib\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from more_itertools import unique_everseen\n",
    "import matplotlib as mpl\n",
    "from scipy.stats import ttest_ind\n",
    "from statistics import mean, stdev, variance\n",
    "from typing import Sequence, List\n",
    "from collections import OrderedDict\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.formula.api import ols, mixedlm\n",
    "%matplotlib notebook\n",
    "import pystan\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load in data and clean for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "username = getpass.getuser()\n",
    "root = pathlib.Path(os.path.join(os.path.join(os.path.expanduser('~')), 'Documents/GitHub/amplification-race-binary-ddm-model') + \"/\")\n",
    "\n",
    "old = False\n",
    "\n",
    "if old == False:\n",
    "    raw = pd.read_csv(root / \"final_dataset_for_ddm_dec_21.csv\")\n",
    "else:\n",
    "    raw = pd.read_csv(root / \"final_dataset_ratings_with_identity_values.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjects with no variation: []\n"
     ]
    }
   ],
   "source": [
    "# create new dataframe df with only the columns that I need for DDM\n",
    "df = pd.DataFrame({'ID': raw['Random.ID'], 'choice': raw['emo_binary'],\n",
    "                   'rt': raw['rt'], 'valence': raw['valence'],\n",
    "                   'identity': raw['faces'], 'intensity': raw['valence_values'],\n",
    "                   'ratio': raw['b_person_ratio']})\n",
    "\n",
    "# convert the nominal variables into integers for Stan; convert identity and intensity strings to lists; convert rt to seconds\n",
    "df = df.dropna(axis=0).reset_index(drop=True)\n",
    "df['choice'] = [1 if x == 'Not Emotional' else 2 for x in df['choice']]\n",
    "df['valence'] = [1 if x == 'Happy' else 2 for x in df['valence']]\n",
    "if old == True:\n",
    "    df['identity'] = [eval(x) for x in df['identity']]\n",
    "    df['intensity'] = [eval(x) for x in df['intensity']]\n",
    "else:\n",
    "    df['identity'] = [x.split(', ') for x in df['identity']]\n",
    "    df['intensity'] = [x.split(', ') for x in df['intensity']]\n",
    "df['rt'] = [x/1000 for x in df['rt']]\n",
    "\n",
    "# remove any subjects that lack variability in their emo_binary choices - can't use them for DDM\n",
    "dellist = []\n",
    "for x in df['ID'].unique():\n",
    "    if len(df[df['ID']==x]['choice'].unique()) < 2:\n",
    "        dellist.append(x)\n",
    "print('subjects with no variation: %s' % dellist)\n",
    "df = df[~df['ID'].isin(dellist)]\n",
    "\n",
    "# remove any observations where rt < 100ms - these are likely false starts\n",
    "df = df[df['rt'] > 0.1]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# collapse face identities into either black or white; append NAs to identity lists that are len < 12 so that all vectors are len 12\n",
    "# likewise, append 0.0 to intensity lists so that all intensity vectors are len 12\n",
    "identitydict = {'E': 1, 'F': 1, 'B': 2, 'C': 2, 'NA': 0}\n",
    "for i, x in enumerate(df['identity']):\n",
    "    while len(x) < 12:\n",
    "        x.append('NA')\n",
    "        df['intensity'][i].append(0)\n",
    "    df['identity'][i] = [identitydict[e] for e in x]\n",
    "    \n",
    "# convert df variables into arrays and vectors for Stan data block\n",
    "grouped = df.groupby(['ID'], sort=False)\n",
    "trials_per = grouped.size()\n",
    "subs = list(trials_per.index)\n",
    "nsubs = len(subs)\n",
    "tsubs = list(trials_per)\n",
    "tmax = max(tsubs)\n",
    "choice = np.full((nsubs, tmax), -1, dtype=int)\n",
    "rt = np.full((nsubs, tmax), -1, dtype=float)\n",
    "valence = np.full((nsubs, tmax), -1, dtype=int)\n",
    "intensity = np.full((nsubs, tmax, 12), -1, dtype=int)\n",
    "identity = np.full((nsubs, tmax, 12), -1, dtype=int)\n",
    "ratio = np.full((nsubs, tmax), -1, dtype=float)\n",
    "sub_group = iter(grouped)\n",
    "for s in range(nsubs):\n",
    "    _, sub_data = next(sub_group)\n",
    "    t = tsubs[s]\n",
    "    choice[s][:t] = sub_data['choice']\n",
    "    rt[s][:t] = sub_data['rt']\n",
    "    valence[s][:t] = sub_data['valence']\n",
    "    intensity[s][:t] = np.asarray([np.array(x) for x in sub_data['intensity']])\n",
    "    identity[s][:t] = np.asarray([np.array(x) for x in sub_data['identity']])\n",
    "    ratio[s][:t] = sub_data['ratio']\n",
    "rtmin = np.full(nsubs, -1, dtype=float)\n",
    "rtbound = 0.1\n",
    "sub_group = iter(grouped)\n",
    "for s in range(nsubs):\n",
    "    _, sub_data = next(sub_group)\n",
    "    rtmin[s] = min(sub_data['rt'])        \n",
    "data = {\n",
    "    'N': nsubs,\n",
    "    'T': tmax,\n",
    "    'Tsub': tsubs,\n",
    "    'choice': choice,\n",
    "    'valence': valence,\n",
    "    'rt': rt,\n",
    "    'rtmin': rtmin,\n",
    "    'rtbound': rtbound,\n",
    "    'intensity': intensity,\n",
    "    'identity': identity,\n",
    "    'ratio': ratio,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1205\n",
      "1115\n",
      "7727\n"
     ]
    }
   ],
   "source": [
    "print(list(df[df['ratio']==0.25][df['valence']==1]['choice']).count(1)) #valence1=happy choice 1=noemotion\n",
    "print(list(df[df['ratio']==0.75][df['valence']==1]['choice']).count(2))\n",
    "print(list(df['choice']).count(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66960.0,\n",
       " 94490.0,\n",
       " 54365.0,\n",
       " 36460.0,\n",
       " 73581.0,\n",
       " 82163.0,\n",
       " 53171.0,\n",
       " 81591.0,\n",
       " 43640.0]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(oldids).difference(newids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "newids = [x for x in list(df['ID'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldids = [float(str(x)+'.0') for x in list(df['ID'].unique())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate the Stan model with code of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Stan code from facesddm_stan.py; remove and reimport if already imported - this is helpful when making changes to Stan code\n",
    "if \"facesddm_stan\" in sys.modules:\n",
    "    sys.modules.pop('facesddm_stan')\n",
    "from facesddm_stan import facesddmcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL DDM_f45104a379bf3a395fd5160683dc7ea9 NOW.\n"
     ]
    }
   ],
   "source": [
    "# compile C++ code for model\n",
    "ddm_sm = pystan.StanModel(model_code=facesddmcode, model_name='DDM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump({'model':ddm_sm}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Start sampling with MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "WARNING:pystan:50 of 50 iterations ended with a divergence (100 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n",
      "WARNING:pystan:Chain 1: E-BFMI = 0.0157\n",
      "WARNING:pystan:E-BFMI below 0.2 indicates you may need to reparameterize your model\n"
     ]
    }
   ],
   "source": [
    "# fit the model to the data\n",
    "# more iterations the better, but at cost of compute and memory; shoot for 100,000 iterations, 50,000 of which are warmup\n",
    "# Use at least 4 chains; thin the samples so that only every other 5 are included in posterior, to reduce autocorrelation\n",
    "# Set a seed for reproducability\n",
    "ddm_fit = ddm_sm.sampling(data=data, iter=500, warmup=250, chains=1, thin=5, refresh=1, seed=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save the model and fit object as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"facesddm.pkl\", \"wb\") as f:\n",
    "    pickle.dump({'model' : ddm_sm, 'fit' : ddm_fit}, f, protocol=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
